import { create } from 'zustand';
import { RealtimeWsClient } from '../rtc/RealtimeWsClient';
import { AudioStreamer } from '../rtc/AudioStreamer';
import { Pcm24Player } from '../rtc/PcmPlayer';
import { voiceChatApi } from '../api/voiceChat';

export interface TranscriptMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
}

export type VoiceCallStatus = 'idle' | 'connecting' | 'connected' | 'error';

interface VoiceCallState {
  // State
  status: VoiceCallStatus;
  isMuted: boolean;
  isAISpeaking: boolean;
  messages: TranscriptMessage[];
  error: string | null;

  // Audio instances
  wsClient: RealtimeWsClient | null;
  audioStreamer: AudioStreamer | null;
  pcmPlayer: Pcm24Player | null;

  // Current session info
  userId: string | null;
  sessionId: string | null;
  voiceSessionId: string | null; // è¯­éŸ³ä¼šè¯IDï¼ˆç”¨äºŽä¿å­˜åˆ°æ•°æ®åº“ï¼‰

  // Actions
  startCall: (userId: string, sessionId?: string) => Promise<void>;
  endCall: () => void;
  toggleMute: () => void;
  handleInterrupt: () => void;
  addMessage: (role: 'user' | 'assistant', content: string, saveToDb?: boolean) => void;
  setStatus: (status: VoiceCallStatus) => void;
  setError: (error: string | null) => void;
  setAISpeaking: (speaking: boolean) => void;
}

export const useVoiceCallStore = create<VoiceCallState>((set, get) => ({
  // Initial state
  status: 'idle',
  isMuted: false,
  isAISpeaking: false,
  messages: [],
  error: null,
  wsClient: null,
  audioStreamer: null,
  pcmPlayer: null,
  userId: null,
  sessionId: null,
  voiceSessionId: null,

  // Actions
  startCall: async (userId: string, sessionId?: string) => {
    const state = get();
    
    // å¦‚æžœå·²ç»åœ¨é€šè¯ä¸­ï¼Œå…ˆç»“æŸå½“å‰é€šè¯
    if (state.status !== 'idle') {
      state.endCall();
    }

    try {
      set({ 
        status: 'connecting', 
        error: null, 
        userId, 
        sessionId: sessionId || `session_${Date.now()}`,
        messages: []
      });

      // åˆ›å»ºè¯­éŸ³ä¼šè¯ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
      let voiceSessionId: string | null = null;
      try {
        const voiceSession = await voiceChatApi.createSession(sessionId);
        voiceSessionId = voiceSession.id;
        set({ voiceSessionId });
        console.log('âœ… è¯­éŸ³ä¼šè¯å·²åˆ›å»º:', voiceSessionId);
      } catch (err) {
        console.error('åˆ›å»ºè¯­éŸ³ä¼šè¯å¤±è´¥:', err);
        // ä¸é˜»å¡žé€šè¯ï¼Œç»§ç»­æ‰§è¡Œ
      }

      // è¯·æ±‚éº¦å…‹é£Žæƒé™
      let stream: MediaStream;
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (err: any) {
        console.error('éº¦å…‹é£Žæƒé™é”™è¯¯:', err);
        let errorMessage = 'æ— æ³•è®¿é—®éº¦å…‹é£Ž';
        if (err.name === 'NotAllowedError') {
          errorMessage = 'è¯·å…è®¸ä½¿ç”¨éº¦å…‹é£Žä»¥è¿›è¡Œè¯­éŸ³é€šè¯';
        } else if (err.name === 'NotFoundError') {
          errorMessage = 'æœªæ£€æµ‹åˆ°éº¦å…‹é£Žè®¾å¤‡';
        }
        set({ status: 'error', error: errorMessage });
        return;
      }

      // åˆ›å»º PcmPlayer å®žä¾‹
      const pcmPlayer = new Pcm24Player();
      pcmPlayer.setSampleRateHz(24000);

      // åˆ›å»º WebSocket å®¢æˆ·ç«¯
      const wsClient = new RealtimeWsClient({
        onOpen: () => {
          console.log('âœ… WebSocket è¿žæŽ¥å·²å»ºç«‹');
          set({ status: 'connected' });
        },
        onClose: (code, reason) => {
          console.log('âŒ WebSocket è¿žæŽ¥å·²å…³é—­', code, reason);
          const currentState = get();
          if (currentState.status !== 'idle') {
            set({ status: 'error', error: 'è¿žæŽ¥å·²æ–­å¼€' });
          }
        },
        onError: (err) => {
          console.error('WebSocket é”™è¯¯:', err);
          set({ status: 'error', error: 'WebSocket è¿žæŽ¥å¤±è´¥' });
        },
        onMessage: (data) => {
          const currentState = get();
          if (!data || typeof data !== 'object') return;

          // å¤„ç†éŸ³é¢‘å“åº”
          if (data.type === 'response.audio.delta' && data.delta) {
            currentState.setAISpeaking(true);
            pcmPlayer.playBase64Pcm24(data.delta).catch((err) => {
              console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', err);
            });
          }

          // å¤„ç†éŸ³é¢‘å“åº”å®Œæˆ
          if (data.type === 'response.audio.done') {
            currentState.setAISpeaking(false);
          }

          // å¤„ç†æ–‡æœ¬è½¬å½•
          if (data.type === 'response.audio_transcript.delta' && data.delta) {
            // ç´¯ç§¯è½¬å½•æ–‡æœ¬
            const lastMessage = currentState.messages[currentState.messages.length - 1];
            if (lastMessage && lastMessage.role === 'assistant') {
              // æ›´æ–°æœ€åŽä¸€æ¡æ¶ˆæ¯
              set({
                messages: [
                  ...currentState.messages.slice(0, -1),
                  {
                    ...lastMessage,
                    content: lastMessage.content + data.delta,
                  },
                ],
              });
            } else {
              // åˆ›å»ºæ–°æ¶ˆæ¯
              currentState.addMessage('assistant', data.delta);
            }
          }

          // å¤„ç†ç”¨æˆ·è¾“å…¥è½¬å½•
          if (data.type === 'conversation.item.input_audio_transcription.completed' && data.transcript) {
            currentState.addMessage('user', data.transcript);
          }

          // å¤„ç†é”™è¯¯
          if (data.type === 'error') {
            console.error('AI æ¨¡åž‹é”™è¯¯:', data.error);
            set({ error: data.error?.message || 'AI æœåŠ¡é”™è¯¯' });
          }
        },
      });

      // è¿žæŽ¥ WebSocket
      const model = 'qwen3-omni-flash-realtime';
      const voice = 'default';
      wsClient.connect(model, voice);

      // åˆ›å»º AudioStreamer å®žä¾‹
      const audioStreamer = new AudioStreamer({
        sampleRateOut: 16000,
        appendMs: 200,
        mode: 'vad',
        enableClientVAD: true, // å¯ç”¨å®¢æˆ·ç«¯ VAD ç”¨äºŽæ‰“æ–­
        sendJson: (payload) => {
          wsClient.sendJson(payload);
        },
        onUserSpeaking: () => {
          // ç”¨æˆ·å¼€å§‹è¯´è¯ï¼Œè§¦å‘æ‰“æ–­
          const currentState = get();
          if (currentState.isAISpeaking) {
            console.log('ðŸ›‘ æ£€æµ‹åˆ°ç”¨æˆ·è¯´è¯ï¼Œæ‰“æ–­ AI');
            currentState.handleInterrupt();
          }
        },
        onError: (err) => {
          console.error('AudioStreamer é”™è¯¯:', err);
        },
      });

      // å¼€å§‹éŸ³é¢‘æ•èŽ·
      await audioStreamer.start(stream);

      set({ 
        wsClient, 
        audioStreamer, 
        pcmPlayer,
      });

    } catch (err: any) {
      console.error('å¯åŠ¨é€šè¯å¤±è´¥:', err);
      set({ 
        status: 'error', 
        error: err.message || 'å¯åŠ¨é€šè¯å¤±è´¥' 
      });
    }
  },

  endCall: () => {
    const state = get();

    // ç»“æŸè¯­éŸ³ä¼šè¯ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    if (state.voiceSessionId) {
      voiceChatApi.endSession(state.voiceSessionId)
        .then(() => {
          console.log('âœ… è¯­éŸ³ä¼šè¯å·²ç»“æŸ');
        })
        .catch((err) => {
          console.error('ç»“æŸè¯­éŸ³ä¼šè¯å¤±è´¥:', err);
        });
    }

    // åœæ­¢éŸ³é¢‘æ•èŽ·
    if (state.audioStreamer) {
      state.audioStreamer.stop().catch(console.error);
    }

    // å…³é—­ WebSocket
    if (state.wsClient) {
      state.wsClient.close();
    }

    // åœæ­¢éŸ³é¢‘æ’­æ”¾
    if (state.pcmPlayer) {
      state.pcmPlayer.stopAll();
    }

    // é‡ç½®çŠ¶æ€
    set({
      status: 'idle',
      isMuted: false,
      isAISpeaking: false,
      error: null,
      wsClient: null,
      audioStreamer: null,
      pcmPlayer: null,
      userId: null,
      sessionId: null,
      voiceSessionId: null,
    });
  },

  toggleMute: () => {
    const state = get();
    const newMutedState = !state.isMuted;

    if (state.audioStreamer) {
      if (newMutedState) {
        // é™éŸ³ï¼šåœæ­¢éŸ³é¢‘æ•èŽ·
        state.audioStreamer.stop().catch(console.error);
      } else {
        // å–æ¶ˆé™éŸ³ï¼šé‡æ–°å¼€å§‹éŸ³é¢‘æ•èŽ·
        navigator.mediaDevices.getUserMedia({ audio: true })
          .then((stream) => {
            if (state.audioStreamer) {
              state.audioStreamer.start(stream).catch(console.error);
            }
          })
          .catch((err) => {
            console.error('æ¢å¤éŸ³é¢‘æ•èŽ·å¤±è´¥:', err);
            set({ error: 'æ— æ³•æ¢å¤éº¦å…‹é£Ž' });
          });
      }
    }

    set({ isMuted: newMutedState });
  },

  handleInterrupt: () => {
    const state = get();

    // ç«‹å³åœæ­¢ AI éŸ³é¢‘æ’­æ”¾
    if (state.pcmPlayer) {
      state.pcmPlayer.stopAll();
    }

    // å‘é€æ‰“æ–­ä¿¡å·åˆ°åŽç«¯
    if (state.wsClient) {
      state.wsClient.sendJson({
        type: 'response.cancel',
        event_id: `event_${Date.now()}`,
      });
    }

    set({ isAISpeaking: false });
  },

  addMessage: (role: 'user' | 'assistant', content: string, saveToDb: boolean = true) => {
    const state = get();
    const newMessage: TranscriptMessage = {
      id: `msg_${Date.now()}_${Math.random().toString(36).slice(2)}`,
      role,
      content,
      timestamp: Date.now(),
    };
    set({ messages: [...state.messages, newMessage] });

    // è‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
    if (saveToDb && state.voiceSessionId) {
      voiceChatApi.saveTranscription(state.voiceSessionId, role, content)
        .then(() => {
          console.log('âœ… è½¬å½•å·²ä¿å­˜:', role, content.substring(0, 50));
        })
        .catch((err) => {
          console.error('ä¿å­˜è½¬å½•å¤±è´¥:', err);
          // ä¸å½±å“ç”¨æˆ·ä½“éªŒï¼Œé™é»˜å¤±è´¥
        });
    }
  },

  setStatus: (status: VoiceCallStatus) => {
    set({ status });
  },

  setError: (error: string | null) => {
    set({ error });
  },

  setAISpeaking: (speaking: boolean) => {
    set({ isAISpeaking: speaking });
  },
}));
